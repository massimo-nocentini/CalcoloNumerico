\section{Before Partial Pivoting}

\begin{exercise}[3.6]
Per il testo dell'esercizio consultare il libro di testo.
\end{exercise}
\begin{proof}
Il numero di operazioni che compio \`e dato da:
\begin{displaymath}
\sum_{i = 1}^{n}{\left( (n-i) + 2(n-i)^{2} \right)} =
\sum_{k = 0}^{n - 1}{\left( k + 2k^{2} \right)} = 
\sum_{k = 0}^{n - 1}{k } + 2\sum_{k = 0}^{n - 1}{k^{2}} 
\end{displaymath}
Con la prima somma si sommano i primi $n-1$ numeri naturali (escludendo lo
zero), per la seconda somma posso applicare il suggerimento del testo 
$\sum_{i = 0}^{n-1}{i^{2}} = \frac{n(n-1)(2n-1)}{6}$, ottengo quindi:
\begin{displaymath}
\begin{split}
\sum_{k = 0}^{n - 1}{k } + 2\sum_{k = 0}^{n - 1}{k^{2}} &= \frac{n(n-1)}{2} +
\frac{2n(n-1)(2n-1)}{6} = n(n-1)\left( \frac{1}{2} + \frac{2n-1}{3}\right ) = \\
&= n(n-1) \frac{4n+1}{6} = (n^{2}-n) \frac{4n+1}{6} = \frac{4n^{3} -3n^{2}-n}{6}
\approx \frac{2}{3}n^{3}
\end{split}
\end{displaymath}
Considerando per l'ultima approssimazione solo il termine di grado massimo.
\end{proof}

\begin{exercise}[3.7, 3.8]
Per i testi degli esercizi consultare il libro di testo.
\end{exercise}
Vedere il codice \nameref{subsection:LUmethod}.

\begin{exercise}[3.9, Lemma 3.4]
Se $A$ \`e diagonale dominante per righe allora lo sono anche tutte le sue
sotto-matrici principali.
\end{exercise}
\begin{proof}
Per ipotesi $A$ \`e diagonale dominante per righe, quindi posso costruire questa 
disuguaglianza:
\begin{displaymath}
|a_{ii}| > \sum_{j = 1 \\j \not = i}^{n}{|a_{ij}|} \geq \sum_{j = 1 \\j \not =
i}^{k}{|a_{ij}|}, \quad k \leq n
\end{displaymath}
con $k$ indice della sotto-matrice di ordine $k$. La disuguaglianza a destra
dimostra l'asserto e la prova \`e terminata.
\end{proof}

\begin{exercise}[3.9, Lemma 3.5]
$A$ \`e diagonale dominante per righe sse $A^{T}$ \`e diagonale dominante per
colonne.
\end{exercise}
\begin{proof}[Proof of $\Rightarrow$]
Per ipotesi $A$ \`e diagonale dominante per righe, ovvero:
\begin{displaymath}
|a_{ii}| > \sum_{j = 1 \\j \not = i}^{n}{|a_{ij}|}
\end{displaymath}
Costruisco la trasposta: se $a_{ij} \in A$ allora $a_{ji} \in A^{T}$. Riscrivo
la definizione precedente:
\begin{displaymath}
|a_{ii}| > \sum_{j = 1 \\j \not = i}^{n}{|a_{ji}|}
\end{displaymath}
Ma questa \`e la definizione di matrice diagonale dominante per colonne e questo
termina la prova.
\end{proof}

\begin{proof}[Proof of $\Leftarrow$]
Con argomento simmetrico si dimostra anche l'implicazione inversa.
\end{proof}

\begin{exercise}[3.10]
Se $A = LDL^{T}$ allora $A$ \`e sdp.
\end{exercise}
\begin{proof}
Devo dimostrare che $A$ soddisfa la definizione sdp:
\begin{itemize}
\item $A = LDL^{T} = (L^{T})^{T}DL^{T} = LDL^{T} = A^{T}$, quindi $A$ \`e
simmetrica.
\item $\forall \vect{x} \in \mathbb{R}^{n}, \vect{x} \not = \vect{0}$ allora
deve valere:
\begin{displaymath}
\vect{x}^{T}A\vect{x} = \vect{x}^{T}LDL^{T}\vect{x} > 0
\end{displaymath}
Costruisco il vettore $\vect{y} = L^{T}\vect{x}$, di conseguenza $\vect{y}^{T}
= \vect{x}^{T}(L^{T})^{T} = \vect{x}^{T}L$. Quindi posso riscrivere
$\vect{y}^{T}D\vect{y} > 0$. Rappresento il prodotto $\vect{y}^{T}(D\vect{y})$:
\begin{displaymath}
\vect{y}^{T}(D\vect{y}) = 
\begin{bmatrix}
y_{1} & \cdots & y_{n}
\end{bmatrix}
\begin{bmatrix}
d_{11} \\
 & d_{22} \\
 & 		& \ddots \\
 & 		&		& \ddots\\
 &  	&  		&		& d_{nn}
\end{bmatrix}
\begin{bmatrix}
y_{1}\\
\vdots \\
y_{n}
\end{bmatrix} = 
\begin{bmatrix}
y_{1} & \cdots & y_{n}
\end{bmatrix}
\begin{bmatrix}
y_{1}d_{11}\\
\vdots \\
y_{n}d_{nn}
\end{bmatrix} =
\sum_{i = 1}^{n}{y_{i}d_{ii}} \geq 0
\end{displaymath} 
In quanto per ipotesi $d_{ii} > 0$. Per avere la somma uguale a 0, si dovrebbe
avere $\vect{y} = \vect{0}$. Ma questo non \`e possibile in quanto $\vect{y}  
= L^{T}\vect{x}$ con $L$ non singolare e $\vect{x} \not = \vect{0}$ per
definizione di sdp. Quindi si ha:
\begin{displaymath}
\sum_{i = 1}^{n}{y_{i}d_{ii}} > 0
\end{displaymath} 
questo \`e quanto richiesta dalla definizione di sdp, quindi $A$ \`e sdp e
questo termina la prova.
\end{itemize}
\end{proof}

\begin{exercise}[3.11]
\label{exercise:311}
Se $A$ \`e non singolare allora $A^{T}A$ e $AA^{T}$ sono sdp.
\end{exercise}
\begin{proof}
Devo verifica che $B = A^{T}A$ sia sdp quindi:
\begin{itemize}
  \item $B = A^{T}A = A^{T}(A^{T})^{T} = A^{T}A = B^{T}$, quindi $B$ \`e
  simmetrica
  \item $\forall \vect{x} \in \mathbb{R}^{n}, \vect{x} \not = \vect{0}$ allora
deve valere:
\begin{displaymath}
\vect{x}^{T}B\vect{x} = \vect{x}^{T}A^{T}A\vect{x} > 0
\end{displaymath}
Costruisco il vettore $\vect{y} = A\vect{x}$, di conseguenza $\vect{y}^{T}
= \vect{x}^{T}A^{T}$. Quindi posso riscrivere
$\vect{y}^{T}\vect{y} > 0$ e rappresento:
\begin{displaymath}
\vect{y}^{T}\vect{y} = 
\begin{bmatrix}
y_{1} & \cdots & y_{n}
\end{bmatrix}
\begin{bmatrix}
y_{1}\\
\vdots \\
y_{n}
\end{bmatrix} =
\sum_{i = 1}^{n}{y_{i}^{2}} \geq 0
\end{displaymath} 
Per avere la somma uguale a 0, si dovrebbe
avere $\vect{y} = \vect{0}$. Ma questo non \`e possibile in quanto $\vect{y}  
= A\vect{x}$ con $A$ non singolare, quindi $A$ non \`e la matrice nulla, e
$\vect{x} \not = \vect{0}$ per definizione di sdp. Quindi si ha:
\begin{displaymath}
\sum_{i = 1}^{n}{y_{i}^{2}} > 0
\end{displaymath} 
questo \`e quanto richiesta dalla definizione di sdp, quindi $B$ \`e sdp.

Dato che ho dimostrato che $B$ \`e simmetrica allora anche $B^{T}$ \`e sdp e
questo termina la prova.
\end{itemize}
\end{proof}

\begin{exercise}[3.12]
Se $A \in \mathbb{R}^{m \times n}$, con $m \geq n = rank(A)$ allora $A^{T}A$ \`e
sdp.
\end{exercise}
\begin{proof}
Devo dimostrare che $B = A^{T}A$ \`e sdp.
\begin{itemize}
  \item $B$ per la prova della simmetria dell'esercizio
  \ref{exercise:311}.
  \item $\forall \vect{x} \in \mathbb{R}^{n}, \vect{x} \not = \vect{0}$ allora
  posso costruire un vettore (come fatto nell'esercizio precedente) $\vect{y} =
  A\vect{x}, \vect{y} \in \mathbb{R}^{m}$ ed il rispettivo $\vect{y}^{T} = 
  \vect{x}^{T}A^{T}, \vect{y}^{T} \in \mathbb{R}^{1 \times m}$ e costruire il 
  prodotto richiesto dalla definizione
  \begin{displaymath}
  	\sum_{i = 1}^{m}{y_{i}^{2}} \geq 0
  \end{displaymath}
  Dato che in $A$ ci sono $m-n$ linearmente dipendenti, allora alcuni termini
  della sommatoria $y_{i}$ possono essere nulli. Ma per le ipotesi, $n =
  rank(A)$, quindi esiste un minore principale $\det(A_{k}) \not = 0$. Per come
  \`e costruito $\vect{y}$ allora $\exists i \in \mathbb{N}: y_{i} \not = 0$
  perch\`e $\det(A_{k}) \not = 0$ per ipotesi e $\vect{x} \not = \vect{0}$ per
  la definizione di sdp. Quindi la somma \`e strettamente positiva e questo
  termina la prova.
\end{itemize}
\end{proof}

\begin{exercise}
Sia $S \in \mathbb{R}^{n \times n}$. Se $\det(S) \not = 0$ allora $A = SS^{T}$
\`e sdp.
\end{exercise}
\begin{proof}
Devo dimostrare che $A$ soddisfa le due richieste della definizione di
matrice sdp.
\begin{itemize}
\item $A = SS^{T} = (S^{T})^{T}S^{T} = SS^{T} = A^{T}$, quindi $A$ \`e
simmetrica.
  \item $\forall \vect{x} \in \mathbb{R}^{n}, \vect{x} \not = \vect{0}$ allora
  \begin{displaymath}
\vect{x}^{T}A\vect{x} = \vect{x}^{T}SS^{T}\vect{x} > 0
\end{displaymath}
Costruisco il vettore $\vect{y} = S^{T}\vect{x}$, di conseguenza $\vect{y}^{T}
= \vect{x}^{T}S$. Quindi posso riscrivere
$\vect{y}^{T}\vect{y} > 0$.
Svolgendo il prodotto riga-colonna vale:
\begin{displaymath}
  	\sum_{i = 1}^{n}{y_{i}^{2}} > 0
  \end{displaymath}
  Questa somma non pu\`o essere zero in quanto per come \`e costruito
  $\vect{y}$, per ipotesi $S$ \`e non singolare e dato che $S = S^{T}$,  anche
  $S^{T}$ \`e non singolare, quindi $S^{T}$ non annulla il prodotto. Rimane da
  controllare il vettore $\vect{x}$, ma per costruzione $\vect{x} \not =
  \vect{0}$,  quindi la somma \`e positiva e questo termina la prova che $A =
  SS^{T}$ \`e sdp.
  \end{itemize}
\end{proof}
\begin{oss}
Posso utilizzare l'asserto del precedente esercizio per costruire una matrice
sdp partendo da una matrice non singolare.
\end{oss}

\begin{exercise}[3.13]
Se $A \in \mathbb{R}^{n \times n}$ allora $A = \frac{1}{2}(A + A^{T}) +
\frac{1}{2}(A - A^{T})$
\end{exercise}
\begin{proof}
\begin{displaymath}
\begin{split}
\frac{1}{2}
\left (
\begin{bmatrix}
a_{11} & \cdots & \cdots &\cdots & a_{1n} \\
\vdots & \ddots &		&		& \vdots\\
\vdots &  		& \ddots & 		& \vdots\\
\vdots & 		&		& \ddots & \vdots\\
a_{n1} & \cdots & \cdots &\cdots & a_{nn}
\end{bmatrix} + 
\begin{bmatrix}
a_{11} & \cdots & \cdots &\cdots & a_{n1} \\
\vdots & \ddots &		&		& \vdots\\
\vdots &  		& \ddots & 		& \vdots\\
\vdots & 		&		& \ddots & \vdots\\
a_{1n} & \cdots & \cdots &\cdots & a_{nn}
\end{bmatrix} \right ) = \\
= \frac{1}{2} 
\begin{bmatrix}
2a_{11} & \cdots & \cdots &\cdots & a_{1n} + a_{n1} \\
\vdots & \ddots &		&		& \vdots\\
\vdots &  		& \ddots & 		& \vdots\\
\vdots & 		&		& \ddots & \vdots\\
a_{n1} + a_{1n} & \cdots & \cdots &\cdots & 2a_{nn}
\end{bmatrix} = A^{s}
\end{split}
\end{displaymath}
La matrice $A_{s}$ \`e una matrice simmetrica.

Analogamente per la differenza:
\begin{displaymath}
\begin{split}
\frac{1}{2}
\left (
\begin{bmatrix}
a_{11} & \cdots & \cdots &\cdots & a_{1n} \\
\vdots & \ddots &		&		& \vdots\\
\vdots &  		& \ddots & 		& \vdots\\
\vdots & 		&		& \ddots & \vdots\\
a_{n1} & \cdots & \cdots &\cdots & a_{nn}
\end{bmatrix} - 
\begin{bmatrix}
a_{11} & \cdots & \cdots &\cdots & a_{n1} \\
\vdots & \ddots &		&		& \vdots\\
\vdots &  		& \ddots & 		& \vdots\\
\vdots & 		&		& \ddots & \vdots\\
a_{1n} & \cdots & \cdots &\cdots & a_{nn}
\end{bmatrix} \right ) = \\
= \frac{1}{2} 
\begin{bmatrix}
0 & \cdots & \cdots &\cdots & a_{1n} - a_{n1} \\
\vdots & \ddots &		&		& \vdots\\
\vdots &  		& \ddots & 		& \vdots\\
\vdots & 		&		& \ddots & \vdots\\
a_{n1} - a_{1n}& \cdots & \cdots &\cdots & 0
\end{bmatrix} = A^{a}
\end{split}
\end{displaymath}
Verifico che $A_{s} + A_{a} = A$:
\begin{displaymath}
\begin{split}
A_{s} + A_{a} = 
\frac{1}{2}
\begin{bmatrix}
2a_{11} & \cdots & \cdots &\cdots & a_{1n} + a_{n1} + a_{1n} - a_{n1}\\
\vdots & \ddots &		&		& \vdots\\
\vdots &  		& \ddots & 		& \vdots\\
\vdots & 		&		& \ddots & \vdots\\
a_{n1} + a_{1n} + a_{n1} - a_{1n} & \cdots & \cdots &\cdots & 2a_{nn}
\end{bmatrix} = \\
= \frac{1}{2}
\begin{bmatrix}
2a_{11} & \cdots & \cdots &\cdots & 2a_{1n}\\
\vdots & \ddots &		&		& \vdots\\
\vdots &  		& \ddots & 		& \vdots\\
\vdots & 		&		& \ddots & \vdots\\
2a_{n1}& \cdots & \cdots &\cdots & 2a_{nn}
\end{bmatrix} = 
\begin{bmatrix}
a_{11} & \cdots & \cdots &\cdots & a_{1n} \\
\vdots & \ddots &		&		& \vdots\\
\vdots &  		& \ddots & 		& \vdots\\
\vdots & 		&		& \ddots & \vdots\\
a_{n1} & \cdots & \cdots &\cdots & a_{nn}
\end{bmatrix} = A
\end{split}
\end{displaymath}
In generale vale $a'_{ij} = a_{ij} + a_{ji} + a_{ij} - a_{ji} = 2a_{ij}$.
L'uguaglianza \`e verificata e questo termina la prova.
\end{proof}

\begin{exercise}[3.16, 3.17]
Per i testi degli esercizi consultare il libro di testo.
\end{exercise}
Vedere il codice \nameref{subsection:LDLmethod}.

\begin{exercise}[3.18]
Per i testi degli esercizi consultare il libro di testo.
\end{exercise}
\begin{lstlisting}
octave:43> A = [
> 1 1 1 1
> 1 2 2 2
> 1 2 1 1
> 1 2 1 2];
octave:45> [L,U,xs] = LDLmethod(A, [1;1;1;1])
error: A non sdp.
\end{lstlisting}

\begin{exercise}
Usare le fattorizzazioni $LU, LDL^{T}$ alla matrice:
\begin{displaymath}
\begin{bmatrix}
1 & -1 & 0 & 0 \\
-1 & 2 & -1 & 0 \\
0 & -1 & 2 & -1 \\
0 & 0 & -1 & 2
\end{bmatrix}
\end{displaymath}
\end{exercise}

\begin{lstlisting}
octave:49> A = [1 -1 0 0; -1 2 -1 0; 0 -1 2 -1; 0 0 -1 2]
A =
   1  -1   0   0
  -1   2  -1   0
   0  -1   2  -1
   0   0  -1   2
octave:50> [L,U,xs] = LUmethod(A, [1;1;1;1])
L =
   1   0   0   0
  -1   1   0   0
   0  -1   1   0
   0   0  -1   1
U =
   1  -1   0   0
   0   1  -1   0
   0   0   1  -1
   0   0   0   1
xs =
   10
    9
    7
    4
octave:51> [L,D,xs] = LDLmethod(A, [1;1;1;1])
L =
   1   0   0   0
  -1   1   0   0
   0  -1   1   0
   0   0  -1   1
D =
   1   0   0   0
   0   1   0   0
   0   0   1   0
   0   0   0   1
xs =
   10
    9
    7
    4
\end{lstlisting}

