\section{Before Partial Pivoting}

\begin{exercise}[3.6]
Per il testo dell'esercizio consultare il libro di testo.
\end{exercise}
\begin{proof}
Il numero di operazioni che compio \`e dato da:
\begin{displaymath}
\sum_{i = 1}^{n}{\left( (n-i) + 2(n-i)^{2} \right)} =
\sum_{k = 0}^{n - 1}{\left( k + 2k^{2} \right)} = 
\sum_{k = 0}^{n - 1}{k } + 2\sum_{k = 0}^{n - 1}{k^{2}} 
\end{displaymath}
Con la prima somma si sommano i primi $n-1$ numeri naturali (escludendo lo
zero), per la seconda somma posso applicare il suggerimento del testo 
$\sum_{i = 0}^{n-1}{i^{2}} = \frac{n(n-1)(2n-1)}{6}$, ottengo quindi:
\begin{displaymath}
\begin{split}
\sum_{k = 0}^{n - 1}{k } + 2\sum_{k = 0}^{n - 1}{k^{2}} &= \frac{n(n-1)}{2} +
\frac{2n(n-1)(2n-1)}{6} = n(n-1)\left( \frac{1}{2} + \frac{2n-1}{3}\right ) = \\
&= n(n-1) \frac{4n+1}{6} = (n^{2}-n) \frac{4n+1}{6} = \frac{4n^{3} -3n^{2}-n}{6}
\approx \frac{2}{3}n^{3}
\end{split}
\end{displaymath}
Considerando per l'ultima approssimazione solo il termine di grado massimo.
\end{proof}

\begin{exercise}[3.7, 3.8]
Per i testi degli esercizi consultare il libro di testo.
\end{exercise}
Vedere il codice \nameref{subsection:LUmethod}.

\begin{oss}
Posso vedere il $k$-esimo vettore di Gauss e la relativa matrice elementare come 
delle funzioni:
\begin{displaymath}
\begin{split}
\vect{g}_{k} = g_{k}(\vect{v}) &= \frac{1}{v_{kk}} 
\begin{bmatrix}
0 \\ \vdots \\ 0 \\ v_{k + 1} \\ \vdots \\ v_{n}
\end{bmatrix} \quad , g_{k}: \mathbb{R}^{n} \rightarrow
\mathbb{R}^{n} \\
L = L(\vect{v}) &= I - g_{k}(\vect{v})\vect{e}_{k}^{T}  \quad , L:
\mathbb{R}^{n \times n} \rightarrow \mathbb{R}^{n \times n}
\end{split}
\end{displaymath}
Inoltre al passo $i$-esimo non si modifica la riga $i$: si modificano gli
elementi sotto $a_{ii}$ e tutta la sottomatrice $A(i + 1:n, i + 1:n)$ (scritta
in notazione Octave).
\\\\
L'equazione del testo 3.22 scrive:
\begin{displaymath}
L = I + \vect{g}_{1}\vect{e}_{1}^{T} + \cdots + \vect{g}_{n-1}\vect{e}_{n-1}^{T}
= \begin{bmatrix}
1 \\
g_{21} & 1 \\
\vdots & \ddots		& \ddots \\
g_{n1} & \cdots		&	g_{n,n-1}	& 1
\end{bmatrix}
\end{displaymath}
con 
\begin{displaymath}
g_{ik} = \vect{e}_{i}^{T} g_{k}(A^{(k)}\vect{e}_{k}) = 
\vect{e}_{i}^{T} \left ( \frac{1}{a_{kk}^{(k)}}
\begin{bmatrix}
0 \\ \vdots \\ 0 \\ a_{k + 1, k}^{(k)} \\ \vdots \\ a_{n k}^{(k)}
\end{bmatrix} \right )
\end{displaymath}
\end{oss}

\begin{exercise}[3.9, Lemma 3.4]
Se $A$ \`e diagonale dominante per righe allora lo sono anche tutte le sue
sotto-matrici principali.
\end{exercise}
\begin{proof}
Per ipotesi $A$ \`e diagonale dominante per righe, quindi posso costruire questa 
disuguaglianza:
\begin{displaymath}
|a_{ii}| > \sum_{j = 1 \\j \not = i}^{n}{|a_{ij}|} \geq \sum_{j = 1 \\j \not =
i}^{k}{|a_{ij}|}, \quad k \leq n
\end{displaymath}
con $k$ indice della sotto-matrice di ordine $k$. La disuguaglianza a destra
dimostra l'asserto e la prova \`e terminata.
\end{proof}

\begin{exercise}[3.9, Lemma 3.5]
$A$ \`e diagonale dominante per righe sse $A^{T}$ \`e diagonale dominante per
colonne.
\end{exercise}
\begin{proof}[Proof of $\Rightarrow$]
Per ipotesi $A$ \`e diagonale dominante per righe, ovvero:
\begin{displaymath}
|a_{ii}| > \sum_{j = 1 \\j \not = i}^{n}{|a_{ij}|}
\end{displaymath}
Costruisco la trasposta: se $a_{ij} \in A$ allora $a_{ji} \in A^{T}$. Riscrivo
la definizione precedente:
\begin{displaymath}
|a_{ii}| > \sum_{j = 1 \\j \not = i}^{n}{|a_{ji}|}
\end{displaymath}
Ma questa \`e la definizione di matrice diagonale dominante per colonne e questo
termina la prova.
\end{proof}

\begin{proof}[Proof of $\Leftarrow$]
Con argomento simmetrico si dimostra anche l'implicazione inversa.
\end{proof}

\begin{exercise}[3.10]
Se $A = LDL^{T}$ allora $A$ \`e sdp.
\end{exercise}
\begin{proof}
Devo dimostrare che $A$ soddisfa la definizione sdp:
\begin{itemize}
\item $A = LDL^{T} = (L^{T})^{T}DL^{T} = LDL^{T} = A^{T}$, quindi $A$ \`e
simmetrica.
\item $\forall \vect{x} \in \mathbb{R}^{n}, \vect{x} \not = \vect{0}$ allora
deve valere:
\begin{displaymath}
\vect{x}^{T}A\vect{x} = \vect{x}^{T}LDL^{T}\vect{x} > 0
\end{displaymath}
Costruisco il vettore $\vect{y} = L^{T}\vect{x}$, di conseguenza $\vect{y}^{T}
= \vect{x}^{T}(L^{T})^{T} = \vect{x}^{T}L$. Quindi posso riscrivere
$\vect{y}^{T}D\vect{y} > 0$. Rappresento il prodotto $\vect{y}^{T}(D\vect{y})$:
\begin{displaymath}
\vect{y}^{T} D \vect{y} = 
\begin{bmatrix}
y_{1} & \cdots & y_{n}
\end{bmatrix}
\begin{bmatrix}
d_{11} \\
 & d_{22} \\
 & 		& \ddots \\
 & 		&		& \ddots\\
 &  	&  		&		& d_{nn}
\end{bmatrix}
\begin{bmatrix}
y_{1}\\
\vdots \\
y_{n}
\end{bmatrix} = 
\begin{bmatrix}
y_{1} & \cdots & y_{n}
\end{bmatrix}
\begin{bmatrix}
y_{1}d_{11}\\
\vdots \\
y_{n}d_{nn}
\end{bmatrix} =
\sum_{i = 1}^{n}{y_{i}^{2}d_{ii}} \geq 0
\end{displaymath} 
In quanto per ipotesi $d_{ii} > 0$. Per avere la somma uguale a 0, si dovrebbe
avere $\vect{y} = \vect{0}$. Ma questo non \`e possibile in quanto $\vect{y}  
= L^{T}\vect{x}$ con $L$ non singolare e $\vect{x} \not = \vect{0}$ per
definizione di sdp. Quindi si ha:
\begin{displaymath}
\vect{y}^{T} D \vect{y} = \sum_{i = 1}^{n}{y_{i}^{2}d_{ii}} > 0
\end{displaymath} 
questo \`e quanto richiesto dalla definizione di sdp, quindi $A$ \`e sdp e
questo termina la prova.
\end{itemize}
\end{proof}

\begin{exercise}[3.11]
\label{exercise:311}
Se $A$ \`e non singolare allora $A^{T}A$ e $AA^{T}$ sono sdp.
\end{exercise}
\begin{proof}
Devo verifica che $B = A^{T}A$ sia sdp quindi:
\begin{itemize}
  \item $B = A^{T}A = A^{T}(A^{T})^{T} = A^{T}A = B^{T}$, quindi $B$ \`e
  simmetrica
  \item $\forall \vect{x} \in \mathbb{R}^{n}, \vect{x} \not = \vect{0}$ allora
deve valere:
\begin{displaymath}
\vect{x}^{T}B\vect{x} = \vect{x}^{T}A^{T}A\vect{x} > 0
\end{displaymath}
Costruisco il vettore $\vect{y} = A\vect{x}$, di conseguenza $\vect{y}^{T}
= \vect{x}^{T}A^{T}$. Quindi posso riscrivere
$\vect{y}^{T}\vect{y} > 0$ e rappresento:
\begin{displaymath}
\vect{y}^{T}\vect{y} = 
\begin{bmatrix}
y_{1} & \cdots & y_{n}
\end{bmatrix}
\begin{bmatrix}
y_{1}\\
\vdots \\
y_{n}
\end{bmatrix} =
\sum_{i = 1}^{n}{y_{i}^{2}} \geq 0
\end{displaymath} 
Per avere la somma uguale a 0, si dovrebbe
avere $\vect{y} = \vect{0}$. Ma questo non \`e possibile in quanto $\vect{y}  
= A\vect{x}$ con $A$ non singolare, quindi $A$ non \`e la matrice nulla, e
$\vect{x} \not = \vect{0}$ per definizione di sdp. Quindi si ha:
\begin{displaymath}
\sum_{i = 1}^{n}{y_{i}^{2}} > 0
\end{displaymath} 
questo \`e quanto richiesta dalla definizione di sdp, quindi $B$ \`e sdp.

Dato che ho dimostrato che $B$ \`e simmetrica allora anche $B^{T}$ \`e sdp e
questo termina la prova.
\end{itemize}
\end{proof}

\begin{exercise}[3.12]
Se $A \in \mathbb{R}^{m \times n}$, con $m \geq n = rank(A)$ allora $A^{T}A$ \`e
sdp.
\end{exercise}
\begin{proof}
Devo dimostrare che $B = A^{T}A$ \`e sdp.
\begin{itemize}
  \item $B$ \`e simmetrica per la prova della simmetria dell'esercizio
  \ref{exercise:311}.
  \item $\forall \vect{x} \in \mathbb{R}^{n}, \vect{x} \not = \vect{0}$ allora
  posso costruire un vettore (come fatto nell'esercizio precedente) $\vect{y} =
  A\vect{x}, \vect{y} \in \mathbb{R}^{m}$ ed il rispettivo $\vect{y}^{T} = 
  \vect{x}^{T}A^{T}, \vect{y}^{T} \in \mathbb{R}^{1 \times m}$ e costruire il 
  prodotto richiesto dalla definizione
  \begin{displaymath}
  	\sum_{i = 1}^{m}{y_{i}^{2}} \geq 0
  \end{displaymath}
  Dato che in $A$ ci sono $m-n$ linearmente dipendenti, allora alcuni termini
  della sommatoria $y_{i}$ possono essere nulli. Ma per le ipotesi, $n =
  rank(A)$, quindi esiste un minore principale $\det(A_{k}) \not = 0$. Per come
  \`e costruito $\vect{y}$ allora $\exists i \in \mathbb{N}: y_{i} \not = 0$
  perch\`e $\det(A_{k}) \not = 0$ per ipotesi e $\vect{x} \not = \vect{0}$ per
  la definizione di sdp. Quindi la somma \`e strettamente positiva e questo
  termina la prova.
\end{itemize}
\end{proof}

\begin{exercise}
Sia $S \in \mathbb{R}^{n \times n}$. Se $\det(S) \not = 0$ allora $A = SS^{T}$
\`e sdp.
\end{exercise}
\begin{proof}
Devo dimostrare che $A$ soddisfa le due richieste della definizione di
matrice sdp.
\begin{itemize}
\item $A = SS^{T} = (S^{T})^{T}S^{T} = SS^{T} = A^{T}$, quindi $A$ \`e
simmetrica.
  \item $\forall \vect{x} \in \mathbb{R}^{n}, \vect{x} \not = \vect{0}$ allora
  \begin{displaymath}
\vect{x}^{T}A\vect{x} = \vect{x}^{T}SS^{T}\vect{x} > 0
\end{displaymath}
Costruisco il vettore $\vect{y} = S^{T}\vect{x}$, di conseguenza $\vect{y}^{T}
= \vect{x}^{T}S$. Quindi posso riscrivere
$\vect{y}^{T}\vect{y} > 0$.
Svolgendo il prodotto riga-colonna vale:
\begin{displaymath}
  	\sum_{i = 1}^{n}{y_{i}^{2}} > 0
  \end{displaymath}
  Questa somma non pu\`o essere zero in quanto per come \`e costruito
  $\vect{y}$, per ipotesi $S$ \`e non singolare e dato che $S = S^{T}$,  anche
  $S^{T}$ \`e non singolare, quindi $S^{T}$ non annulla il prodotto. Rimane da
  controllare il vettore $\vect{x}$, ma per costruzione $\vect{x} \not =
  \vect{0}$,  quindi la somma \`e positiva e questo termina la prova che $A =
  SS^{T}$ \`e sdp.
  \end{itemize}
\end{proof}
\begin{oss}
Posso utilizzare l'asserto del precedente esercizio per costruire una matrice
sdp partendo da una matrice non singolare.
\end{oss}

\begin{exercise}[3.13a]
Se $A \in \mathbb{R}^{n \times n}$ allora $A = \frac{1}{2}(A + A^{T}) +
\frac{1}{2}(A - A^{T}) = A_{s} + A_{a}$
\end{exercise}
\begin{proof}
\begin{displaymath}
\begin{split}
\frac{1}{2}
\left (
\begin{bmatrix}
a_{11} & \cdots & \cdots &\cdots & a_{1n} \\
\vdots & \ddots &		&		& \vdots\\
\vdots &  		& \ddots & 		& \vdots\\
\vdots & 		&		& \ddots & \vdots\\
a_{n1} & \cdots & \cdots &\cdots & a_{nn}
\end{bmatrix} + 
\begin{bmatrix}
a_{11} & \cdots & \cdots &\cdots & a_{n1} \\
\vdots & \ddots &		&		& \vdots\\
\vdots &  		& \ddots & 		& \vdots\\
\vdots & 		&		& \ddots & \vdots\\
a_{1n} & \cdots & \cdots &\cdots & a_{nn}
\end{bmatrix} \right ) = \\
= \frac{1}{2} 
\begin{bmatrix}
2a_{11} & \cdots & \cdots &\cdots & a_{1n} + a_{n1} \\
\vdots & \ddots &		&		& \vdots\\
\vdots &  		& \ddots & 		& \vdots\\
\vdots & 		&		& \ddots & \vdots\\
a_{n1} + a_{1n} & \cdots & \cdots &\cdots & 2a_{nn}
\end{bmatrix} = A^{s}
\end{split}
\end{displaymath}
La matrice $A_{s}$ \`e una matrice simmetrica.

Analogamente per la differenza:
\begin{displaymath}
\begin{split}
\frac{1}{2}
\left (
\begin{bmatrix}
a_{11} & \cdots & \cdots &\cdots & a_{1n} \\
\vdots & \ddots &		&		& \vdots\\
\vdots &  		& \ddots & 		& \vdots\\
\vdots & 		&		& \ddots & \vdots\\
a_{n1} & \cdots & \cdots &\cdots & a_{nn}
\end{bmatrix} - 
\begin{bmatrix}
a_{11} & \cdots & \cdots &\cdots & a_{n1} \\
\vdots & \ddots &		&		& \vdots\\
\vdots &  		& \ddots & 		& \vdots\\
\vdots & 		&		& \ddots & \vdots\\
a_{1n} & \cdots & \cdots &\cdots & a_{nn}
\end{bmatrix} \right ) = \\
= \frac{1}{2} 
\begin{bmatrix}
0 & \cdots & \cdots &\cdots & a_{1n} - a_{n1} \\
\vdots & \ddots &		&		& \vdots\\
\vdots &  		& \ddots & 		& \vdots\\
\vdots & 		&		& \ddots & \vdots\\
a_{n1} - a_{1n}& \cdots & \cdots &\cdots & 0
\end{bmatrix} = A^{a}
\end{split}
\end{displaymath}
Verifico che $A_{s} + A_{a} = A$:
\begin{displaymath}
\begin{split}
A_{s} + A_{a} = 
\frac{1}{2}
\begin{bmatrix}
2a_{11} & \cdots & \cdots &\cdots & a_{1n} + a_{n1} + a_{1n} - a_{n1}\\
\vdots & \ddots &		&		& \vdots\\
\vdots &  		& \ddots & 		& \vdots\\
\vdots & 		&		& \ddots & \vdots\\
a_{n1} + a_{1n} + a_{n1} - a_{1n} & \cdots & \cdots &\cdots & 2a_{nn}
\end{bmatrix} = \\
= \frac{1}{2}
\begin{bmatrix}
2a_{11} & \cdots & \cdots &\cdots & 2a_{1n}\\
\vdots & \ddots &		&		& \vdots\\
\vdots &  		& \ddots & 		& \vdots\\
\vdots & 		&		& \ddots & \vdots\\
2a_{n1}& \cdots & \cdots &\cdots & 2a_{nn}
\end{bmatrix} = 
\begin{bmatrix}
a_{11} & \cdots & \cdots &\cdots & a_{1n} \\
\vdots & \ddots &		&		& \vdots\\
\vdots &  		& \ddots & 		& \vdots\\
\vdots & 		&		& \ddots & \vdots\\
a_{n1} & \cdots & \cdots &\cdots & a_{nn}
\end{bmatrix} = A
\end{split}
\end{displaymath}
In generale vale $a'_{ij} = a_{ij} + a_{ji} + a_{ij} - a_{ji} = 2a_{ij}$.
L'uguaglianza \`e verificata e questo termina la prova.
\end{proof}


\begin{exercise}[3.13b]
Se $A \in \mathbb{R}^{n \times n}$ e $\forall \vect{x} \in \mathbb{R}^{n}$
allora $\vect{x}^{T}A\vect{x} = \vect{x}^{T}A_{s}\vect{x}$
\end{exercise}
\begin{proof}
Per la parte $3.13a$ vale che $A = A_{s} + A_{a}$, quindi posso sostituire ed
applicare la propriet\`a distributiva del prodotto:
\begin{displaymath}
\vect{x}^{T}A\vect{x} = \vect{x}^{T}(A_{s} + A_{a})\vect{x} =
\vect{x}^{T}A_{s}\vect{x} + \vect{x}^{T}A_{a}\vect{x}
\end{displaymath}
Posso ridurre l'argomento che si vuole dimostrare ($\vect{x}^{T}A\vect{x} =
\vect{x}^{T}A_{s}\vect{x}$), considerandolo vero, a dimostrare che
necessariamente deve valere $0 = \vect{x}^{T}A_{a}\vect{x}$.

Per definizione di $A_{a}$ vale $\frac{1}{2}(A - A^{T}) = A_{a}$, che posso
rappresentare:
\begin{displaymath}
\frac{1}{2} 
\begin{bmatrix}
0 & a_{12} - a_{21} & \cdots &\cdots & a_{1n} - a_{n1} \\
a_{21} - a_{12} & 0 &		&		& \vdots\\
\vdots &  		& \ddots & 		& \vdots\\
\vdots & 		&		& \ddots & \vdots\\
a_{n1} - a_{1n}& \cdots & \cdots &\cdots & 0
\end{bmatrix} = A^{a}
\end{displaymath}
Fisso un vettore $\vect{x} \in \mathbb{R}^{n}$ e rappresento il prodotto
$\vect{x}^{T}A_{a}\vect{x}$:
\begin{displaymath}
\begin{bmatrix}
x_{1} & \cdots & x_{n}
\end{bmatrix}
\begin{bmatrix}
0 & a_{12} - a_{21} & \cdots &\cdots & a_{1n} - a_{n1} \\
a_{21} - a_{12} & 0 &		&		& \vdots\\
\vdots &  		& \ddots & 		& \vdots\\
\vdots & 		&		& \ddots & \vdots\\
a_{n1} - a_{1n}& \cdots & \cdots &\cdots & 0
\end{bmatrix} 
\begin{bmatrix}
x_{1} \\ 
\vdots \\ 
\vdots \\
\vdots \\
x_{n}
\end{bmatrix}= 2 A^{a}
\end{displaymath}
Sviluppo il prodotto matrice-vettore colonna:
\begin{displaymath}
\begin{bmatrix}
x_{1} & \cdots & x_{n}
\end{bmatrix}
\begin{bmatrix}
0x_{1} + (a_{12} - a_{21})x_{2} + (a_{13} - a_{31})x_{3} + \cdots + (a_{1n} -
a_{n1})x_{n} \\ 
(a_{21} - a_{12})x_{1} + 0x_{2} + (a_{23} - a_{32})x_{3} + \cdots + (a_{2n} -
a_{n2})x_{n} \\ 
\vdots\\
(a_{n1} - a_{1n})x_{1} + (a_{n2} - a_{2n})x_{2} + (a_{n3} - a_{3n})x_{3} +
\cdots + 0x_{n}
\end{bmatrix} = 2 A^{a}
\end{displaymath}
Sviluppo il prodotto vettore riga-vettore colonna ottenendo uno scalare:
\begin{displaymath}
\begin{array}{c}
0x_{1}x_{1} + (a_{12} - a_{21})x_{2}x_{1} + (a_{13} - a_{31})x_{3}x_{1} + \cdots
+ (a_{1n} - a_{n1})x_{n}x_{1} +\\ 
+ (a_{21} - a_{12})x_{1}x_{2} + 0x_{2}x_{2} + (a_{23} - a_{32})x_{3}x_{2} +
\cdots + (a_{2n} - a_{n2})x_{n}x_{2} + \\ 
\vdots\\
+ (a_{n1} - a_{1n})x_{1}x_{n} + (a_{n2} - a_{2n})x_{2}x_{n} + (a_{n3} -
a_{3n})x_{3}x_{n} + \cdots + 0x_{n}x_{n}
\end{array} = 2 A^{a}
\end{displaymath}
Omettendo gli elementi che contengono i fattori $a x_{i} x_{i} = 0$ ottengo:
\begin{displaymath}
0 = \begin{array}{c}
(a_{12} - a_{21})x_{2}x_{1} + (a_{13} - a_{31})x_{3}x_{1} + \cdots
+ (a_{1n} - a_{n1})x_{n}x_{1} +\\ 
+ (a_{21} - a_{12})x_{1}x_{2} + (a_{23} - a_{32})x_{3}x_{2} +
\cdots + (a_{2n} - a_{n2})x_{n}x_{2} + \\ 
\vdots\\
+ (a_{n1} - a_{1n})x_{1}x_{n} + (a_{n2} - a_{2n})x_{2}x_{n} + (a_{n3} -
a_{3n})x_{3}x_{n} + \cdots + (a_{n,n-1} - a_{n-1,n})x_{n-1}x_{n} 
\end{array} = 2 A^{a}
\end{displaymath}
Ma $(a_{ij} - a_{ji})x_{j}x_{i} = -(a_{ji}-a_{ij})x_{i}x_{j}$ quindi tutti gli
elementi della somma precedente si eliminano a coppie. Per completezza dimostro
che il numero di elementi della precedente somma \`e pari. 

Gli elementi della penultima matrice (comprendendo anche i prodotti nulli) \`e
pari al prodotto cartesiano $n^{2}$, sottraggo gli elementi nulli, ovvero la
diagonale, quindi gli elementi non nulli sono $n^{2} - n$. Devo dimostrare
quindi $n^{2} - n = 2k$, con $k \in \mathbb{N}$. Divido per 2 entrambi i membri
ottenendo $\frac{n(n - 1)}{2} = k$. Il primo membro \`e la somma dei primi $n-1$
numeri naturali, quindi $\sum_{i=1}^{n-1}{i} = k$. Ma dato che l'indice
superiore della somma \`e finito ($n-1 <\infty$) allora la somma ammette un
valore finito $k \in \mathbb{N}$, l'uguaglianza \`e verificata e questo conclude
la prova.
\end{proof}

\begin{exercise}[3.16, 3.17]
Per i testi degli esercizi consultare il libro di testo.
\end{exercise}
Vedere il codice \nameref{subsection:LDLmethod}.

\begin{exercise}[3.18]
Per i testi degli esercizi consultare il libro di testo.
\end{exercise}
\begin{lstlisting}
octave:43> A = [
> 1 1 1 1
> 1 2 2 2
> 1 2 1 1
> 1 2 1 2];
octave:45> [L,U,xs] = LDLmethod(A, [1;1;1;1])
error: A non sdp.
\end{lstlisting}

\begin{exercise}
Usare le fattorizzazioni $LU, LDL^{T}$ alla matrice:
\begin{displaymath}
\begin{bmatrix}
1 & -1 & 0 & 0 \\
-1 & 2 & -1 & 0 \\
0 & -1 & 2 & -1 \\
0 & 0 & -1 & 2
\end{bmatrix}
\end{displaymath}
\end{exercise}

\begin{lstlisting}
octave:49> A = [1 -1 0 0; -1 2 -1 0; 0 -1 2 -1; 0 0 -1 2]
A =
   1  -1   0   0
  -1   2  -1   0
   0  -1   2  -1
   0   0  -1   2
octave:50> [L,U,xs] = LUmethod(A, [1;1;1;1])
L =
   1   0   0   0
  -1   1   0   0
   0  -1   1   0
   0   0  -1   1
U =
   1  -1   0   0
   0   1  -1   0
   0   0   1  -1
   0   0   0   1
xs =
   10
    9
    7
    4
octave:51> [L,D,xs] = LDLmethod(A, [1;1;1;1])
L =
   1   0   0   0
  -1   1   0   0
   0  -1   1   0
   0   0  -1   1
D =
   1   0   0   0
   0   1   0   0
   0   0   1   0
   0   0   0   1
xs =
   10
    9
    7
    4
\end{lstlisting}

\begin{exercise}[3.19]
$a^{(i)}_{ii} \in A^{(i+1)} \in \mathbb{R}^{n \times n}, \quad a^{(i)}_{ii} \not
= 0, \forall i \in {1, \ldots, n}$ se $\det(A) \not = 0$
\end{exercise}
\begin{proof}
Rappresento la matrice $A^{(i+1)}$:	
\begin{displaymath}
A^{(i+1)} = \begin{bmatrix}
a_{k_{1} 1}^{(1)} & \cdots & \cdots &\cdots &\cdots & a_{k_{1} n}^{(1)} \\
0 & \ddots &		&	 &	& \vdots\\
\vdots &  		& a_{k_{i-1}, i-1}^{(i-1)} & 	\cdots	& \cdots & a_{k_{i-1},
n}^{(i-1)}\\ 
\vdots & 		&	0	&	a_{ii}^{(i)}	& \cdots & a_{in}^{(i)}\\
\vdots & 		& \vdots		& \vdots & &\vdots\\
0	   & \cdots & 0 	& a_{ni}^{(i)} &\cdots & a_{nn}^{(i)}
\end{bmatrix} = \lbrace \text{ ragionando a blocchi } \rbrace =  
\begin{array}{c|c}
A_{i-1} & B \\
\hline
0		& C
\end{array}
\end{displaymath}
Adesso posso applicare la funzione ad entrambi i membri pi\`u esterni
dell'uguaglianza, ottengo quindi 
\begin{displaymath}
\det(A^{i+1}) = \det(A_{i-1})\det(C) -
0\det(B) = \det(A_{i-1})\det(C)
\end{displaymath}
Dato che ho potuto raggiungere il passo $i$-esimo della fattorizzazione, allora
significa che $a_{k_{j} j} \not = 0$, con $j \in \lbrace 1, \ldots, i-1
\rbrace$. Osservo che $A_{i-1}$ \`e tringolare inferiore quindi, per l'esercizio 
\ref{exercise:triangularMatrixDet}, vale $\det(A_{i-1}) = a_{k_{1}1}^{(1)}
\cdots a_{k_{i-1} i-1}^{(i-1)} \not = 0$. Ma per ipotesi $A$ \`e
non singolare quindi vale $\det(A^{i+1}) \not = 0$, in quanto sono state $i$
trasformazioni reversibili (basta considerare le $L^{-1}$), ovvero rendono le
due matrici equivalenti. Per questo si ha necessariamente che $\det(C) \not =
0$, ovvero lo colonne della matrice $C$ sono linearmente indipendenti e quindi
\begin{displaymath}
a_{k_{i}i}^{i} = \max_{k \geq i}{|a_{ki}^{(i)}|} \not = 0
\end{displaymath}
in altre parole la colonna $A^{(i+1)}\vect{e}_{i} \not = \vect{0}$.
\end{proof}

