\section{Fattorizzazioni}
\subsection{triangularSystemSolver}
\lstinputlisting{listings/chapterThree/triangularSystemSolver.m}

\subsection{normalizationEngine}
\lstinputlisting{listings/chapterThree/normalizationEngine.m}

\subsection{LUmethod}
\label{subsection:LUmethod}
\lstinputlisting{listings/chapterThree/LUmethod.m}

\subsection{LDLmethod}
\label{subsection:LDLmethod}
\lstinputlisting{listings/chapterThree/LDLmethod.m}

\subsection{PALUmethod}
\label{subsection:PALUmethod}
\lstinputlisting{listings/chapterThree/PALUmethod.m}

\subsection{QRmethod}
\label{subsection:QRmethod}
\begin{oss}[Sull'uso e costruzione delle matrici di eliminazione $H$]
Durante il metodo non vengono create e sviluppate in modo esplicito le matrici
di eliminazione $H^{i}$ per effettuare i prodotti $H^{i}\vect{v}$: vengono
invece costruite in modo implicito, ovvero la computazione che viene effettuata
dal metodo \`e direttamente la trasformazione ortogonale 
\begin{displaymath}
H\vect{v} \rightarrow \vect{v} -
\frac{2}{\vect{z}^{T}\vect{z}}\vect{z}(\vect{z}^{T}\vect{v})
\end{displaymath}
\end{oss}

\begin{oss}[Sul costo del prodotto scalare]
Dati due vettori $\vect{a}, \vect{b} \in \mathbb{R}^{k}$, il prodotto scalare
costa $2k - 1$ operazioni, di cui $k$ per i prodotti $\forall i \in
\{1, \ldots , k \} [a_{i}b_{i}]$, e $k-1$ somme dei $k$ prodotti.
\end{oss}

\begin{oss}[Idee alla base del metodo]
Queste sono le idee alla base del metodo:
\begin{itemize}
  \item $H(\beta \vect{z}) = H(\vect{z}), \forall \beta \in \mathbb{R}$, ovvero
  la matrice di eliminazione non varia per multipli del vettore caratteristico
  $\vect{z}$ di Householder (permette quindi di scegliere $\beta$ in modo
  da avere il vettore caratteristico con una struttura particolare, $z_{1} =
  1$).
  
  \item la seguente ugualianza permette di evitare il calcolo del prodotto
  scalare, risparmiando $2m$ operazioni:
  	\begin{displaymath}
  		-\frac{z_{1}}{\alpha} =
  		\frac{2}{\vect{z}^{T}\vect{z}}
  	\end{displaymath}
  vedi esercizio \ref{exercise:exercise328}.
  
  \item $v_{1} = A(i, i) - \alpha$ con $\alpha A(i, i) < 0$, permette di
  evitare il fenomeno della cancellazione numerica.
\end{itemize}
\end{oss}

\begin{oss}[Sulla struttura delle matrici di eliminazione $H_{k}$]
Ragionando a blocchi sulla struttura delle matrici di eliminazione vale:
\begin{displaymath}
H_{i+1} = \left [ \begin{array}{c|c}
I_{i} 		& 	\vect{0}^{T} \\
\hline
\vect{0}	&	H^{(i+1)}
\end{array} \right ]
\end{displaymath}
con $H^{(i+1)}$ elementare di Householder relativa alla colonna $i+1$ di
$A^{(i)}$:
\begin{displaymath}
A^{(i)}\vect{e}_{i+1} = \begin{bmatrix}
a_{i+1, i+1}^{(i)} \\
\vdots \\
a_{m, i+1}^{(i)}
\end{bmatrix}
\end{displaymath}
Il precedente vettore \`e il vettore che si vuole rendere uguale a
$\alpha\vect{e}_{i+1}$.
\\\\
Al passo $i$-esimo ottengo:
\begin{displaymath}
\begin{split}
A^{(i)} &= H_{i}A^{(i-1)} = H_{i} \cdots H_{1}A  =
\begin{bmatrix}
a_{11}^{(1)} & a_{12}^{(1)} & \cdots & a_{1i}^{(1)}	& \cdots & \cdots &
a_{1n}^{(1)} \\ 
	& a_{22}^{(2)} & \cdots	& a_{2i}^{(2)} & \cdots & \cdots & a_{2n}^{(2)} \\
	&	&	\ddots	& \vdots &	&	& \vdots \\
	&	&	& a_{i-1,i-1}^{(i-1)}	& a_{i-1,i}^{(i-1)} & \cdots & a_{i-1,n}^{(i-1)}  \\
	&	&	& 	&  \\
 	& 	&	&	&  & H^{(i)}A_{ii}^{(i-1)} &  \\
	& 	&	&	& 	&		&  
\end{bmatrix} = \\ 
& = \begin{bmatrix}
a_{11}^{(1)} & a_{12}^{(1)} & \cdots & a_{1i}^{(1)}	& \cdots & \cdots &
a_{1n}^{(1)} \\ 
	& a_{22}^{(2)} & \cdots	& a_{2i}^{(2)} & \cdots & \cdots & a_{2n}^{(2)} \\
	&	&	\ddots	& \vdots &	&	& \vdots \\
	&	&	& a_{ii}^{(i)}	& a_{i,i+1}^{(i)} & \cdots & a_{in}^{(i)}  \\
	&	&	& 	& a_{i+1,i+1}^{(i)} & \cdots & a_{i+1,n}^{(i)}  \\
 	& 	&	&	& \vdots & \ddots & \vdots \\
	& 	&	&	& a_{m, i+1}^{(i)}	&	\cdots	& a_{m, n}^{(i)} 
\end{bmatrix}
\end{split}
\end{displaymath}
Attenzione: al primo passo $a_{11}^{(1)}$ viene aggiornato differentemente per
quanto avviene per la fattorizzazione LU senza pivoting.
\end{oss}

\begin{oss}[Forma matriciale del passo di aggiornamento]
\begin{displaymath}
\begin{bmatrix}
a_{i+1, j}^{(i+1)} \\
\vdots \\
a_{m, j}^{(i+1)}
\end{bmatrix} = 
\begin{bmatrix}
a_{i+1, j}^{(i)} \\
\vdots \\
a_{m, j}^{(i)}
\end{bmatrix} - \frac{2}{(\vect{z}^{(i+1)})^{T} \vect{z}^{(i+1)}} %times
\vect{z}^{(i+1)} %times 
\left ( (\vect{z}^{(i+1)})^{T} \begin{bmatrix}
a_{i+1, j}^{(i+1)} \\
\vdots \\
a_{m, j}^{(i+1)} 
\end{bmatrix} \right )
\end{displaymath}
La precedente uguaglianza rappresenta in forma matriciale l'istruzione
\emph{Octave} per l'aggiornamento della matrice (e implicitamente la
costruzione della matrice triangolare superiore $R$ (no $\hat{R}$)).
\end{oss}

\lstinputlisting{listings/chapterThree/QRmethod.m}

\subsection{functionExercise332}
\label{subsection:functionExercise332}
\lstinputlisting{listings/chapterThree/functionExercise332.m}